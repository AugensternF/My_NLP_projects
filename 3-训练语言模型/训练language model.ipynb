{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练语言模型\n",
    "\n",
    "用RNN,LSTM,GRU来训练一个语言模型，用于预测单词的下一个词\n",
    "- 学习语言模型，以及如何训练一个语言模型\n",
    "- 学习torchtext的基本使用方法\n",
    "    - 构建 vocabulary\n",
    "    - word to inde 和 index to word\n",
    "- 学习torch.nn的一些基本模型\n",
    "    - Linear\n",
    "    - RNN\n",
    "    - LSTM\n",
    "    - GRU\n",
    "- RNN的训练技巧\n",
    "    - Gradient Clipping\n",
    "- 如何保存和读取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们会使用 [torchtext](https://github.com/pytorch/text) 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=red><b>先了解下torchtext库：[torchtext介绍和使用教程](https://blog.csdn.net/u012436149/article/details/79310176)：这个新手必看，不看下面代码听不懂</b></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "EMBEDDING_SIZE = 650  \n",
    "MAX_VOCAB_SIZE = 50000  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们会继续使用上次的text8作为我们的训练，验证和测试数据\n",
    "- torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集\n",
    "- BPTTIterator可以连续地得到连贯的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 50002\n"
     ]
    }
   ],
   "source": [
    "TEXT = torchtext.data.Field(lower=True)   #Field对象：如何预处理文本数据的信息，这里定义单词全部小写\n",
    "# torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "                    path=\".\",  #当前文件夹\n",
    "                    train=\"text8.train.txt\", \n",
    "                    validation=\"text8.dev.txt\", \n",
    "                    test=\"text8.test.txt\", \n",
    "                    text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
    "#build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量。\n",
    "print(\"vocabulary size: {}\".format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two', 'is', 'as', 'eight', 'for', 's', 'five', 'three', 'was', 'by', 'that', 'four', 'six', 'seven', 'with', 'on', 'are', 'it', 'from', 'or', 'his', 'an', 'be', 'this', 'he', 'at', 'which', 'not', 'also', 'have', 'were', 'has', 'but', 'other', 'their', 'its', 'first', 'they', 'had']\n",
      "------------------------------------------------------------\n",
      "[('<unk>', 0), ('<pad>', 1), ('the', 2), ('of', 3), ('and', 4), ('one', 5), ('in', 6), ('a', 7), ('to', 8), ('zero', 9), ('nine', 10), ('two', 11), ('is', 12), ('as', 13), ('eight', 14), ('for', 15), ('s', 16), ('five', 17), ('three', 18), ('was', 19), ('by', 20), ('that', 21), ('four', 22), ('six', 23), ('seven', 24), ('with', 25), ('on', 26), ('are', 27), ('it', 28), ('from', 29), ('or', 30), ('his', 31), ('an', 32), ('be', 33), ('this', 34), ('he', 35), ('at', 36), ('which', 37), ('not', 38), ('also', 39), ('have', 40), ('were', 41), ('has', 42), ('but', 43), ('other', 44), ('their', 45), ('its', 46), ('first', 47), ('they', 48), ('had', 49)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[0:50]) \n",
    "# 这里越靠前越常见，增加x了两个特殊的token，<unk>表示未知的单词，<pad>表示padding。\n",
    "print(\"------\"*10)\n",
    "print(list(TEXT.vocab.stoi.items())[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIterator：标准迭代器\\n\\nBucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，\\n因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，\\n因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\\n除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\\n\\nBPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab) # 50002\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "                            (train, val, test), \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            device=-1, \n",
    "                            bptt_len=50, # 反向传播往回传的长度，这里我暂时理解为一个样本有多少个单词传入模型\n",
    "                            repeat=False, \n",
    "                            shuffle=True)\n",
    "# BPTTIterator可以连续地得到连贯的句子，BPTT的全称是back propagation through time。\n",
    "'''\n",
    "Iterator：标准迭代器\n",
    "BucketIerator：相比于标准迭代器，会将类似长度的样本当做一批来处理，\n",
    "因为在文本处理中经常会需要将每一批样本长度补齐为当前批中最长序列的长度，\n",
    "因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n",
    "除此之外，我们还可以在Field中通过fix_length参数来对样本进行截断补齐操作。\n",
    "BPTTIterator: 基于BPTT(基于时间的反向传播算法)的迭代器，一般用于语言模型中。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 50x32]\n",
      "\t[.target]:[torch.LongTensor of size 50x32]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 50x32]\n",
      "\t[.target]:[torch.LongTensor of size 50x32]\n",
      "\n",
      "[torchtext.data.batch.Batch of size 32]\n",
      "\t[.text]:[torch.LongTensor of size 50x32]\n",
      "\t[.target]:[torch.LongTensor of size 50x32]\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_iter))) # 一个batch训练集维度\n",
    "print(next(iter(val_iter))) # 一个batch验证集维度\n",
    "print(next(iter(test_iter))) # 一个batch测试集维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,1]]))   # 打印一个输入的句子\n",
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,1]])) # 打印一个输出的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the\n",
      "0\n",
      "originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans <unk> of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization\n",
      "1\n",
      "combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility\n",
      "1\n",
      "in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of\n",
      "2\n",
      "culture few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars\n",
      "2\n",
      "few living ainu settlements exist many authentic ainu villages advertised in hokkaido are simply tourist attractions language the ainu language is significantly different from japanese in its syntax phonology morphology and vocabulary although there have been attempts to show that they are related the vast majority of modern scholars reject\n",
      "3\n",
      "zero the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although\n",
      "3\n",
      "the apple iie card an expansion card for the lc line of macintosh computers was released essentially a miniaturized apple iie computer on a card utilizing the mega ii chip from the apple iigs it allowed the macintosh to run eight bit apple iie software through hardware emulation although video\n",
      "4\n",
      "in papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j <unk> one\n",
      "4\n",
      "papers have been written arguing that the anthropic principle would explain the physical constants such as the fine structure constant the number of dimensions in the universe and the cosmological constant the three primary versions of the principle as stated by john d barrow and frank j <unk> one nine\n"
     ]
    }
   ],
   "source": [
    "for j in range(5): # 这种取法是在一个固定的batch里取数据，发现一个batch里的数据是连不起来的。\n",
    "    print(j)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,j].data]))\n",
    "    print(j)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,j].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "all such citadels rome jerusalem celtic bratislava many in asia minor or even castle hill at edinburgh the most famous example of the kind is the acropolis of athens which by reason of its historical associations and the famous buildings erected upon it is generally known without qualification as simply\n",
      "0\n",
      "such citadels rome jerusalem celtic bratislava many in asia minor or even castle hill at edinburgh the most famous example of the kind is the acropolis of athens which by reason of its historical associations and the famous buildings erected upon it is generally known without qualification as simply the\n",
      "1\n",
      "the acropolis because of its classical greco roman style the ruins of mission san juan <unk> s great stone church in california united states have been dubbed the american acropolis other parts of the world developed other names for the high citadel or alc zar which often reinforced a naturally\n",
      "1\n",
      "acropolis because of its classical greco roman style the ruins of mission san juan <unk> s great stone church in california united states have been dubbed the american acropolis other parts of the world developed other names for the high citadel or alc zar which often reinforced a naturally strong\n",
      "2\n",
      "strong site in central italy many small rural communes still cluster at the base of a fortified habitation known as la <unk> of the commune the term acropolis is also used to described the central complex of overlapping structures such as <unk> and pyramids in many mayan cities including tikal\n",
      "2\n",
      "site in central italy many small rural communes still cluster at the base of a fortified habitation known as la <unk> of the commune the term acropolis is also used to described the central complex of overlapping structures such as <unk> and pyramids in many mayan cities including tikal and\n",
      "3\n",
      "and cop n classical studies acupuncture chart from the ming dynasty acupuncture from lat <unk> needle noun and <unk> prick verb or in standard mandarin zh n ji is one of the main branches of traditional chinese medicine or tcm others being herbal medicine and tui na it is a\n",
      "3\n",
      "cop n classical studies acupuncture chart from the ming dynasty acupuncture from lat <unk> needle noun and <unk> prick verb or in standard mandarin zh n ji is one of the main branches of traditional chinese medicine or tcm others being herbal medicine and tui na it is a therapeutic\n",
      "4\n",
      "therapeutic technique from that framework intended to restore health and well being the technique involves the insertion of needles into acupuncture points on the body by trained practitioners the needles most commonly used in present day practice are made of stainless steel and are of approximately the same diameter as\n",
      "4\n",
      "technique from that framework intended to restore health and well being the technique involves the insertion of needles into acupuncture points on the body by trained practitioners the needles most commonly used in present day practice are made of stainless steel and are of approximately the same diameter as a\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): # 这种取法是在每个batch里取某一个相同位置数据，发现不同batch间相同位置的数据是可以连起来的。这里有点小疑问。\n",
    "    batch = next(it)\n",
    "    print(i)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,2].data]))\n",
    "    print(i)\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, vocab_size, embedding_size, hidden_size, nlayers, dropout=0.5):\n",
    "        ''' 该模型包含以下几层:\n",
    "            - 词嵌入层\n",
    "            - 一个循环神经网络层(RNN, LSTM, GRU)\n",
    "            - 一个线性层，从hidden state到输出单词表\n",
    "            - 一个dropout层，用来做regularization\n",
    "        '''\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        if rnn_type in ['LSTM', 'GRU']:          \n",
    "            self.rnn = getattr(nn, rnn_type)(embedding_size, hidden_size, nlayers, dropout=dropout)\n",
    "            # getattr(nn, rnn_type) 相当于 nn.rnn_type\n",
    "            # nlayers代表纵向有多少层。还有个参数是bidirectional: 是否是双向LSTM，默认false\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # 最后线性全连接隐藏层的维度(1000,50002)\n",
    "      \n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):   \n",
    "        ''' Forward pass:\n",
    "            - word embedding\n",
    "            - 输入循环神经网络\n",
    "            - 一个线性层从hidden state转化为输出单词表\n",
    "        '''       \n",
    "        # input.shape = seq_len * batch = [50, 32]，可以在LSTM里定义batch_first = True\n",
    "        # hidden = (nlayers * b * hidden_size)\n",
    "        # hidden是个元组，输入有两个参数，一个是刚开始的隐藏层h的维度，一个是刚开始的用于记忆的c的维度，\n",
    "       \n",
    "        embed = self.drop(self.embedding(input))  #seq_len * b * embedding_size\n",
    "        output, hidden = self.rnn(embed, hidden) \n",
    "        # output.shape = seq_len * b * hidden_size \n",
    "        # hidden元组 = (h层维度：nlayers * 32 * hidden_size, c层维度：nlayers * 32 * hidden_size)\n",
    "        output = self.drop(output)\n",
    "        linear = self.linear(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        # output最后的输出层一定要是二维的，只是为了能进行全连接层的运算，所以把前两个维度拼到一起，（50*32,hidden_size)\n",
    "        # linear.shape=（seq_len*b,hidden_size)*(hidden_size, vocab_size)= [seq_len*b, vocab_size]\n",
    "        \n",
    "        return linear.view(output.size(0), output.size(1), linear.size(1)), hidden\n",
    "               # 我们要知道每一个位置预测的是哪个单词，所以最终输出要恢复维度 = [seq_len, b, vocab_size]\n",
    "               # hidden = (h层维度：nlayers * b * hidden_size, c层维度：nlayers * b * hidden_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, requires_grad=True):\n",
    "        # 最初隐藏层参数的初始化\n",
    "        weight = next(self.parameters())\n",
    "        # weight = torch.Size([50002, 650])是所有参数的第一个参数\n",
    "        # 所有参数self.parameters()，是个生成器，LSTM所有参数维度种类如下：\n",
    "        # print(list(iter(self.parameters())))\n",
    "        # torch.Size([50002, 650])\n",
    "        # torch.Size([4000, 650])\n",
    "        # torch.Size([4000, 1000])\n",
    "        # torch.Size([4000]) # 偏置项\n",
    "        # torch.Size([4000])\n",
    "        # torch.Size([4000, 1000])\n",
    "        # torch.Size([4000, 1000])\n",
    "        # torch.Size([4000])\n",
    "        # torch.Size([4000])\n",
    "        # torch.Size([50002, 1000])\n",
    "        # torch.Size([50002])\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=requires_grad))\n",
    "                   # return = (2 * 32 * 1000, 2 * 32 * 1000)\n",
    "                   # 这里不明白为什么需要weight.new_zeros，我估计是想整个计算图能链接起来\n",
    "                   # 这里特别注意hidden的输入不是model的参数，不参与更新，就跟输入数据x一样                 \n",
    "        else:\n",
    "            return weight.new_zeros((self.nlayers, batch_size, self.hidden_size), requires_grad=requires_grad)\n",
    "            # GRU神经网络把h层和c层合并了，所以这里只有一层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1000 \n",
    "model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, hidden_size, 2, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (embedding): Embedding(50002, 650)\n",
       "  (rnn): LSTM(650, 1000, num_layers=2, dropout=0.5)\n",
       "  (linear): Linear(in_features=1000, out_features=50002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50002, 650])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们首先定义评估模型的代码。\n",
    "- 模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dev_iter):\n",
    "    model.eval() # 预测模式\n",
    "    total_loss = 0.\n",
    "    it = iter(data)\n",
    "    total_count = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "# 这里不管是训练模式还是预测模式，h层的输入都是初始化为0，hidden的输入不是model的参数\n",
    "# 这里model里的model.parameters()已经是训练过的参数。\n",
    "\n",
    "        for i, batch in enumerate(dev_iter):\n",
    "            data, target = batch.text, batch.target\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            hidden = repackage_hidden(hidden)   # 截断计算图\n",
    "            with torch.no_grad():\n",
    "                output, hidden = model(data, hidden)\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            total_count += np.multiply(*data.size())  # 得到该batch的单词总数，*data.szie进行拆包，得到seq_len * batch = 50*32\n",
    "            total_loss += loss.item()*np.multiply(*data.size())  # 一次batch总的损失\n",
    "\n",
    "    loss = total_loss / total_count # 整个验证集总的损失除以总的单词数\n",
    "    model.train() \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.ones((5,3))\n",
    "print(a.size())\n",
    "np.multiply(*a.size()) \n",
    "# *的作用是将list或dict序列拆分为一个个的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将当前隐藏层hidden与之前进行截断\n",
    "def repackage_hidden(hidden):\n",
    "    if isinstance(hidden, torch.Tensor): \n",
    "        # 这个是GRU的截断，因为只有一个隐藏层\n",
    "        return hidden.detach() # 截断计算图，h是全的计算图的开始，只是保留了h的值\n",
    "    else: # 这个是LSTM的截断，有两个隐藏层，格式是元组\n",
    "        return tuple(repackage_hidden(v) for v in hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss function和optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() \n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)\n",
    "# 每调用一次这个函数，lenrning_rate就降一半，0.5就是一半的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iter 0 loss 10.821578979492188\n",
      "best model, val loss:  10.782116411285918\n",
      "epoch 0 iter 1000 loss 6.5122528076171875\n",
      "epoch 0 iter 2000 loss 6.3599748611450195\n",
      "epoch 0 iter 3000 loss 6.13856315612793\n",
      "epoch 0 iter 4000 loss 5.473214626312256\n",
      "epoch 0 iter 5000 loss 5.901871204376221\n",
      "epoch 0 iter 6000 loss 5.85321569442749\n",
      "epoch 0 iter 7000 loss 5.636535167694092\n",
      "epoch 0 iter 8000 loss 5.7489800453186035\n",
      "epoch 0 iter 9000 loss 5.464158058166504\n",
      "epoch 0 iter 10000 loss 5.554863452911377\n",
      "best model, val loss:  5.264891533569864\n",
      "epoch 0 iter 11000 loss 5.703625202178955\n",
      "epoch 0 iter 12000 loss 5.6448974609375\n",
      "epoch 0 iter 13000 loss 5.372857570648193\n",
      "epoch 0 iter 14000 loss 5.2639479637146\n",
      "epoch 1 iter 0 loss 5.696778297424316\n",
      "best model, val loss:  5.124550380139679\n",
      "epoch 1 iter 1000 loss 5.534722805023193\n",
      "epoch 1 iter 2000 loss 5.599489212036133\n",
      "epoch 1 iter 3000 loss 5.459986686706543\n",
      "epoch 1 iter 4000 loss 4.927192211151123\n",
      "epoch 1 iter 5000 loss 5.435710906982422\n",
      "epoch 1 iter 6000 loss 5.4059576988220215\n",
      "epoch 1 iter 7000 loss 5.308575630187988\n",
      "epoch 1 iter 8000 loss 5.405811786651611\n",
      "epoch 1 iter 9000 loss 5.1389055252075195\n",
      "epoch 1 iter 10000 loss 5.226413726806641\n",
      "best model, val loss:  4.946829228873176\n",
      "epoch 1 iter 11000 loss 5.379891395568848\n",
      "epoch 1 iter 12000 loss 5.360724925994873\n",
      "epoch 1 iter 13000 loss 5.176026344299316\n",
      "epoch 1 iter 14000 loss 5.110936641693115\n"
     ]
    }
   ],
   "source": [
    "GRAD_CLIP = 1.\n",
    "NUM_EPOCHS = 2\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() \n",
    "    hidden = model.init_hidden(BATCH_SIZE)  #隐藏层初始化,得到hidden初始化后的维度\n",
    " \n",
    "    for i, batch in enumerate(train_iter):\n",
    "        data, target = batch.text, batch.target\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        \n",
    "# 语言模型每个batch的隐藏层的输出值是要继续作为下一个batch的隐藏层的输入的,\n",
    "# 因为batch数量很多，如果一直往后传，会造成整个计算图很庞大，反向传播会内存崩溃。\n",
    "# 所有每次一个batch的计算图迭代完成后，需要把计算图截断，只保留隐藏层的输出值。\n",
    "# 不过只有语言模型才这么干，其他比如翻译模型不需要这么做。\n",
    "# repackage_hidden自定义函数用来截断计算图的。\n",
    "\n",
    "        optimizer.zero_grad()   \n",
    "        output, hidden = model(data, hidden)  # output = (50,32,50002)\n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))   #loss_fn((num, class), num)\n",
    "# output.view(-1, VOCAB_SIZE) = (1600,50002)\n",
    "# target.view(-1) =(1600),关于pytorch中交叉熵的计算公式请看下面链接。\n",
    "# https://blog.csdn.net/geter_CS/article/details/84857220\n",
    "        loss.backward()\n",
    "        # 防止梯度爆炸，设定阈值，当梯度大于阈值时，更新的梯度为阈值\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP) \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
    "    \n",
    "        if i % 10000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)  #在val_iter上进行验证 \n",
    "            \n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                print(\"best model, val_loss: \", val_loss)\n",
    "                torch.save(model.state_dict(), \"lm-best.th\")\n",
    "            else: # 否则loss没有降下来，需要优化\n",
    "                scheduler.step()  # 自动调整学习率\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "                # 学习率调整后需要更新optimizer，下次训练就用更新后的\n",
    "            val_losses.append(val_loss) # 保存每10000次迭代后的验证集损失损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载保存好的模型参数\n",
    "best_model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, hidden_size, 2, dropout=0.5)\n",
    "if USE_CUDA:\n",
    "    best_model = best_model.cuda()\n",
    "best_model.load_state_dict(torch.load(\"lm-best.th\"))\n",
    "# 把训练好的模型参数load到best_model里"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的模型在valid数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  140.72803934425724\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(best_model, val_iter)\n",
    "print(\"perplexity: \", np.exp(val_loss))\n",
    "# 这里不清楚语言模型的评估指标perplexity = np.exp(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用最好的模型在测试数据上计算perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  178.54742013696125\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_iter)\n",
    "print(\"perplexity: \", np.exp(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的模型生成一些句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s influence clinton decision de gaulle is himself sappho s iv one family banquet was made published by paul <unk> and by a persuaded to prevent arcane of animate poverty based at copernicus bachelor in search services and in a cruise corps references eds the robin series july four one nine zero eight summer gutenberg one nine six four births one nine two eight deaths timeline of this method by the fourth amendment the german ioc known for his <unk> from <unk> one eight nine eight one seven eight nine management was established in one nine seven zero they had\n"
     ]
    }
   ],
   "source": [
    "hidden = best_model.init_hidden(1) # batch_size = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "# (1,1)表示（seq_len, batch_size）输出格式是1行1列的2维tensor，VOCAB_SIZE表示随机取的值小于VOCAB_SIZE=50002\n",
    "# 我们input相当于取的是一个单词\n",
    "words = []\n",
    "for i in range(100):\n",
    "    output, hidden = best_model(input, hidden)\n",
    "    # output.shape = 1 * 1 * 50002\n",
    "    # hidden = (2 * 1 * 1000, 2 * 1 * 1000)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    # .exp()的两个作用：一是把概率更大的变得更大，二是把负数经过e后变成正数，下面.multinomial参数需要正数\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]   #得到的是概率最大的单词的索引,[0]相当于拿到数      \n",
    "    # 按照word_weights里面的概率随机的取值，概率大的取到的机会大。\n",
    "    # torch.multinomial看这个博客理解：https://blog.csdn.net/monchin/article/details/79787621\n",
    "    # 这里如果选择概率最大的，会每次生成重复的句子。\n",
    "    input.fill_(word_idx) # 预测的单词index是word_idx，然后把word_idx作为下一个循环预测的input输入\n",
    "    word = TEXT.vocab.itos[word_idx] # 根据word_idx取出对应的单词\n",
    "    words.append(word) \n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11293]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(50002, (1, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
