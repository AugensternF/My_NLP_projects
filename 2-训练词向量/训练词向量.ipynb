{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量\n",
    "\n",
    "- 用Skip-thought模型训练词向量\n",
    "\n",
    "在这一份notebook中，尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
    "\n",
    "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
    "\n",
    "以下是一些我们没有实现的细节\n",
    "- subsampling：参考论文section 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.utils.data as tud  #Pytorch读取训练集需要用到torch.utils.data类\n",
    "from torch.nn.parameter import Parameter  #参数更新和优化函数\n",
    "from collections import Counter \n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "import pandas as pd\n",
    "import scipy  #提供了许多数学算法和函数的实现\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两个模块的区别：**[torch.nn 和 torch.functional 的区别](https://blog.csdn.net/hawkcici160/article/details/80140059)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对word2vec有了解\n",
    "[CBOW模型理解](https://blog.csdn.net/lilong117194/article/details/81979522)\n",
    "\n",
    "[Skip-Gram模型理解](https://www.jianshu.com/p/da235893e4a5)\n",
    "\n",
    "负例采样就是Skip-Gram模型的输出不是周围词的概率了，是正例和负例的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "# 保证实验结果可以复现\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    "    \n",
    "K = 10 # 负样本随机采样数量\n",
    "C = 3 # 指定周围三个单词进行预测\n",
    "NUM_EPOCHS = 2 \n",
    "VOCAB_SIZE = 30000 \n",
    "BATCH_SIZE = 128 \n",
    "LEARNING_RATE = 0.2 \n",
    "EMBEDDING_SIZE = 100      \n",
    "    \n",
    "LOG_FILE = \"word-embedding.log\"\n",
    "\n",
    "def word_tokenize(text): \n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text8.train.txt\", \"r\") as fin: \n",
    "    text = fin.read() # 一次性读入文件所有内容\n",
    "    \n",
    "text = [w for w in word_tokenize(text.lower())] \n",
    "vocab = dict(Counter(text).most_common(VOCAB_SIZE-1))\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "\n",
    "idx_to_word = [word for word in vocab.keys()] \n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs) # 用来做 negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现Dataloader\n",
    "\n",
    "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
    "为了使用dataloader，我们需要定义以下两个function:\n",
    "\n",
    "- ```__len__``` function需要返回整个数据集中有多少个item\n",
    "- ```__get__``` 根据给定的index返回一个item\n",
    "\n",
    "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader理解：https://blog.csdn.net/qq_36653505/article/details/83351808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingDataset(tud.Dataset): #tud.Dataset父类\n",
    "    \n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):    \n",
    "        super(WordEmbeddingDataset, self).__init__() \n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
    "        #字典 get() 函数返回指定键的值（第一个参数），如果值不在字典中返回默认值（第二个参数）\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        #变成tensor类型，这里变成longtensor，也可以torch.LongTensor(self.text_encoded)\n",
    "        \n",
    "        self.word_to_idx = word_to_idx \n",
    "        self.idx_to_word = idx_to_word  \n",
    "        self.word_freqs = torch.Tensor(word_freqs) \n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.text_encoded) #所有单词的总数\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        center_word = self.text_encoded[idx] \n",
    "\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
    "        \n",
    "        return center_word, pos_words, neg_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader理解：https://blog.csdn.net/qq_36653505/article/details/83351808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 60])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloader))[0].shape) # 一个batch中间词维度data\n",
    "print(next(iter(dataloader))[1].shape) # 一个batch周围词维度\n",
    "print(next(iter(dataloader))[2].shape) # 一个batch负样本维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义PyTorch模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        ''' 初始化输出和输出embedding\n",
    "        '''\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size  #30000\n",
    "        self.embed_size = embed_size  #100\n",
    "        \n",
    "        initrange = 0.5 / self.embed_size\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #模型输出nn.Embedding(30000, 100)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "         #模型输入nn.Embedding(30000, 100)\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        #权重初始化的一种方法\n",
    "        \n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围出现过的单词 [batch_size * (c * 2)]\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (c * 2 * K)]\n",
    "        \n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        \n",
    "        batch_size = input_labels.size(0) \n",
    "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
    "        pos_embedding = self.out_embed(pos_labels) # B * (2C) * embed_size \n",
    "        neg_embedding = self.out_embed(neg_labels) # B * (2*C*K) * embed_size\n",
    "\n",
    "        #torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "        \n",
    "        #下面loss计算就是论文里的公式\n",
    "        log_pos = F.logsigmoid(log_pos).sum(1) # batch_size\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size     \n",
    "        loss = log_pos + log_neg # 正样本损失和负样本损失和尽量最大\n",
    "        \n",
    "        return -loss # 最大转化成最小\n",
    "    \n",
    "    #取出self.in_embed数据参数，维度：（30000,100），就是我们要训练的词向量\n",
    "    # 这里本来模型训练有两个矩阵的，self.in_embed和self.out_embed两个, 只是作者认为输入矩阵比较好，就舍弃了输出矩阵\n",
    "    def input_embeddings(self):   \n",
    "        return self.in_embed.weight.data.cpu().numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filename, embedding_weights): \n",
    "    # embedding_weights是训练之后的embedding向量。\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename, sep=\",\") \n",
    "    else:\n",
    "        data = pd.read_csv(filename, sep=\"\\t\") \n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    \n",
    "    for i in data.iloc[:, 0:2].index: \n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1] \n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            # 在分别取出这两个单词对应的embedding向量，具体为啥是这种取出方式[[word1_idx]]，可以自行研究\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            # 用余弦相似度计算这两个100维向量的相似度。这个是模型算出来的相似度\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "            # 这个是人类统计得到的相似度\n",
    "\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
    "    # 因为相似度是浮点数，不是0 1 这些固定标签值，所以不能用准确度评估指标\n",
    "    # scipy.stats.spearmanr网址：https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "    # scipy.stats.spearmanr评估两个分布的相似度，有两个返回值correlation, pvalue\n",
    "    # correlation是评估相关性的指标（-1，1），越接近1越相关，pvalue值大家可以自己搜索理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss: 45.747718811035156\n"
     ]
    }
   ],
   "source": [
    "for e in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        if USE_CUDA: \n",
    "            input_labels = input_labels.cuda()\n",
    "            pos_labels = pos_labels.cuda()\n",
    "            neg_labels = neg_labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean() \n",
    "        # model返回的是一个batch所有样本的损失，需要求个平均\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout: \n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "                # 训练过程，我没跑，本地肯定跑不动的\n",
    "        \n",
    "        if i % 2000 == 0: \n",
    "            embedding_weights = model.input_embeddings() \n",
    "            # 取出（30000，100）训练的词向量\n",
    "            sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "            sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "            sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "            with open(LOG_FILE, \"a\") as fout:\n",
    "                print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                \n",
    "    embedding_weights = model.input_embeddings() # 调用最终训练好的embeding词向量\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights) # 保存参数\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE)) # 保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"embedding-{}.th\".format(EMBEDDING_SIZE))) # 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 MEN 和 Simplex-999 数据集上做评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex-999 SpearmanrResult(correlation=0.17251697429101504, pvalue=7.863946056740345e-08)\n",
      "men SpearmanrResult(correlation=0.1778096817088841, pvalue=7.565661657312768e-20)\n",
      "wordsim353 SpearmanrResult(correlation=0.27153702278146635, pvalue=8.842165885381714e-07)\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights))\n",
    "print(\"men\", evaluate(\"men.txt\", embedding_weights))\n",
    "print(\"wordsim353\", evaluate(\"wordsim353.csv\", embedding_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['good', 'plutarch', 'earle', 'capo', 'atlantic', 'count', 'ultimate', 'executable', 'kangaroo', 'vh']\n",
      "fresh ['fresh', 'steelers', 'appointment', 'came', 'inspectors', 'annihilation', 'tensor', 'tariffs', 'neg', 'jacobi']\n",
      "monster ['monster', 'rosicrucian', 'moving', 'fascism', 'preparation', 'municipalities', 'fieldwork', 'wasted', 'deterrent', 'forum']\n",
      "green ['green', 'obligations', 'autocephalous', 'sanitary', 'morphology', 'temperament', 'canaanites', 'uppercase', 'guided', 'steadily']\n",
      "like ['like', 'alternated', 'breeds', 'evident', 'grief', 'barriers', 'kino', 'paleo', 'calculus', 'cid']\n",
      "america ['america', 'christus', 'ido', 'greek', 'skillfully', 'plundering', 'juventus', 'dalmatia', 'canine', 'boles']\n",
      "chicago ['chicago', 'bans', 'scarecrow', 'debacle', 'school', 'monomers', 'rendezvous', 'mounted', 'governors', 'magician']\n",
      "work ['work', 'apostles', 'columbian', 'providence', 'buren', 'smoother', 'neutrons', 'legendary', 'predates', 'vedas']\n",
      "computer ['computer', 'brig', 'fitzroy', 'employing', 'screen', 'hippolytus', 'sforza', 'indications', 'wo', 'toxic']\n",
      "language ['language', 'gardener', 'duesberg', 'quantum', 'dirk', 'danforth', 'thunderstorms', 'migratory', 'wound', 'quickly']\n"
     ]
    }
   ],
   "source": [
    "def find_nearest(word):\n",
    "    index = word_to_idx[word] \n",
    "    embedding = embedding_weights[index] # 取出这个单词的embedding向量\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    # 计算所有30000个embedding向量与传入单词embedding向量的相似度距离\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]] # 返回前10个最相似的\n",
    "\n",
    "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "henry\n",
      "charles\n",
      "pope\n",
      "queen\n",
      "iii\n",
      "prince\n",
      "elizabeth\n",
      "alexander\n",
      "constantine\n",
      "edward\n",
      "son\n",
      "iv\n",
      "louis\n",
      "emperor\n",
      "mary\n",
      "james\n",
      "joseph\n",
      "frederick\n",
      "francis\n"
     ]
    }
   ],
   "source": [
    "man_idx = word_to_idx[\"man\"] \n",
    "king_idx = word_to_idx[\"king\"] \n",
    "woman_idx = word_to_idx[\"woman\"]\n",
    "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
    "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "for i in cos_dis.argsort()[:20]:\n",
    "    print(idx_to_word[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
